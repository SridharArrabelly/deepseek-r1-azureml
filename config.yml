config:
    AZURE_SUBSCRIPTION_ID: "xxxx" # Please modify to your subscription
    AZURE_RESOURCE_GROUP: "xxxx" # Please modify to your Azure resource group
    AZURE_WORKSPACE: "xxxx" # Please modify to your Azure workspace
    HF_TOKEN: "xxxx" # Please modify to your Hugging Face token
    HF_MODEL_NAME_OR_PATH: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    IS_DEBUG: true

serve:
    azure_env_name: "llm-serving-vllm-2025-01-28" # Please modify to your AzureML env name
    azure_model_name: "deepseek-r1-distill-v1" # Please modify to your AzureML model name
    azure_endpoint_name: "deepseek-r1-endpoint-v1"
    azure_deployment_name: "deepseek-r1-distill-blue"
    azure_serving_cluster_size: "Standard_NC24ads_A100_v4" # 1 x A100 (80GB)


# $schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
# name: current14
# endpoint_name: vllm-endpoint-2024-11-16
# model: azureml:phi3-finetune-2024-11-05:1
# model_mount_path: /models
# environment_variables:
#     MODEL_NAME: microsoft/Phi-3.5-mini-instruct
#     MODEL_PATH: /models/outputs # replace with your model's folder name 
#     VLLM_ARGS: "--enable-lora --lora-modules phi3-lora=/models/outputs"
# environment:
#     image: 1aaaa0403714478abaafa6042ed1080e.azurecr.io/azureml/azureml_1542387a3a70a1156eaef950dce12be3
#     inference_config:
#         liveness_route:
#             port: 8000
#             path: /health
#         readiness_route:
#             port: 8000
#             path: /health
#         scoring_route:
#             port: 8000
#             path: /
# instance_type: Standard_NC24ads_A100_v4
# instance_count: 1
# request_settings:
#     request_timeout_ms: 10000
#     max_concurrent_requests_per_instance: 4
# liveness_probe:
#     initial_delay: 10
#     period: 10
#     timeout: 2
#     success_threshold: 1
#     failure_threshold: 30
# readiness_probe:
#     initial_delay: 120
#     period: 10
#     timeout: 2
#     success_threshold: 1
#     failure_threshold: 30
    